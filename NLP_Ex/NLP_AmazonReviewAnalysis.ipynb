{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://github.com/melanieshi0120/NLP_Analysis_Amazon_Reviews/blob/master/Amazon_sport_product_review.ipynb<br>\n",
    "data dource: http://jmcauley.ucsd.edu/data/amazon/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from wordcloud import WordCloud\n",
    "import pickle\n",
    "import re, string\n",
    "import sys\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sport_and_outdoor = []\n",
    "for line in open('Sports_and_Outdoors_5.json', 'r'):\n",
    "    sport_and_outdoor.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sport_and_outdoor)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column which combines review and summary\n",
    "df['combined_text'] = df['reviewText'] + ' ' + df['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some reviewrs' names are missing, er will remove reviewName columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many unique values in rating column\n",
    "df['overall'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a target column based on product_rating, if rating <= 3 means bad: 0 if rating >= 4 good:1\n",
    "df['target'] = [0 if x <= 3 else 1 for x in df['overall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column for helpfulness numerator and helpfulness denominator\n",
    "df['helpfulness_Numerator'] = [x[0] for x in df['helpful']]\n",
    "df['helpfulness_Denominator'] = [x[1] for x in df['helpful']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Counts by Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot review counts in different rating group\n",
    "rating = pd.DataFrame(df['overall'].value_counts()).reset_index().sort_values(by='index', ascending=True)\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.barplot(rating['index'], rating['overall'])\n",
    "plt.xticks(fontsize=10)\n",
    "plt.xlabel('Rating Stars', fontsize=15)\n",
    "plt.ylabel('review_counts', fontsize=15)\n",
    "plt.title('Review Counts by Rating', fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot review counts in bad review group and good review group\n",
    "bad_good = pd.DataFrame(df['target'].value_counts()).reset_index().sort_values(by='index', ascending=False)\n",
    "sns.barplot(bad_good['index'], bad_good['target'])\n",
    "plt.xticks([0,1], ['Bad Reviews', 'Good Reviews'], fontsize=15)\n",
    "plt.ylabel('review_counts', fontsize=15)\n",
    "plt.title('Review Counts by Bad.Good Reviews', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of helpfulness_Numerator and helpfulness_Denominator in positive reviews\n",
    "plt.title('distribution of helpfulness of positive Reviews '.title(), fontsize=15)\n",
    "df[df['target']==1]['helpfulness_Numerator'].value_counts()[:20].plot(kind='hist', label='Numerator')\n",
    "df[df['target']==1]['helpfulness_Denominator'].value_counts()[:20].plot(kind='hist', label='Denominator')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of helpfulness_Numerator and helpfulness_Denominator in negative reviews\n",
    "plt.title('distribution of helpfulness of Negative Reviews '.title(), fontsize=15)\n",
    "df[df['target']==0]['helpfulness_Numerator'].value_counts()[:20].plot(kind='hist', label='Numerator')\n",
    "df[df['target']==0]['helpfulness_Denominator'].value_counts()[:20].plot(kind='hist', label='Denominator')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReviewTime and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "df['reviewTime'] = pd.to_datetime(df['reviewTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot how many products sold from 2003 to 2015\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.title('Review Counts by Time', fontsize=20)\n",
    "plt.ylabel('Review Counts', fontsize=10)\n",
    "plt.xlabel('Year', fontsize=10)\n",
    "df[df.target==0]['reviewTime'].value_counts().plot(label='Negative review', color='r', alpha=0.8)\n",
    "df[df.target==1]['reviewTime'].value_counts().plot(label='Positive review', color='b')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# In 2013, Amazon sells over 200 million products in the USA, which are categoried into 35 \n",
    "# departments and almost 20 million in Sports & Outdoors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see in different rating level which products have highest rating - best seller\n",
    "# plot the mean of rating for each product and got top 20 in rating:\n",
    "pd.DataFrame(df.groupby('asin')['overall'].mean()).sort_values(by='overall', ascending=False).iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st :Hydracentials Sporty 25 oz Insulated Stainless Steel Water\n",
    "# 2nd: Table Tennis Racket Cover \n",
    "# 3rd: Drymax Run Mini Crew Socks\n",
    "# 4th: MSR Lightning Flotation Tail\n",
    "# 5th: Tiberius Arms First Strike Paintballs (White, 100 Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wchi products are the best seller regardless their rating\n",
    "best_seller = pd.DataFrame(df['asin'].value_counts()).sort_values(by='asin', ascending=False).iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_seller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Bike\n",
    "# 2nd Impact Sport OD Green Electric Earmuff  \n",
    "# 3rd 50 Feet, Black : Rothco 550lb. Type III Nylon Paracord for yoga \n",
    "# 4th Survival Bundle - LifeStraw Personal Water Filter AND Magnesium Fire Starter \n",
    "# 5th HOPPE'S 24011 BoreSnake Rifle Bore Cleaner, M-16.22-.223 Caliber, 5.56mm\n",
    "#[' Pistol Magazine Loader','Electric Earmuff','Nylon Paracord for yoga','LifeStraw Water Filter','Rifle Bore Cleaner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams before removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2,2), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "#source:https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words_good = get_top_n_bigram(df[df['target']==1]['combined_text'],30)\n",
    "common_words_bad = get_top_n_bigram(df[df['target']==0]['combined_text'],30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,8))\n",
    "# good reviews bigrams\n",
    "plt.subplot(1,2,1)\n",
    "x_good = [x[0] for x in common_words_good]\n",
    "y_good = [x[1] for x in common_words_good]\n",
    "sns.barplot(x_good, y_good, color='g')\n",
    "plt.xticks(rotation=90, fontsize=20)\n",
    "plt.title('Top 20 Bigrams in Positive Reviews', fontsize=20)\n",
    "for i in range(len(x_good)):\n",
    "    plt.text(i-0.2, y_good[i]+100, '{}'.format(y_good[i]), size=15, rotation=45)\n",
    "    \n",
    "plt.subplot(1,2,2)\n",
    "x_bad = [x[0] for x in common_words_bad]\n",
    "y_bad = [x[1] for x in common_words_bad]\n",
    "sns.barplot(x_bad, y_bad, color='b')\n",
    "plt.xticks(rotation=90, fontsize=20)\n",
    "plt.title('Top 20 Bigram in Negative Reviews', fontsize=20)\n",
    "for i in range(len(x_bad)):\n",
    "    plt.text(i-0.2, y_bad[i], '{}'.format(y_bad[i]), size=15, rotation=45)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams before removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_trigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(3,3)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "#source:https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words_good2 = get_top_n_trigram(df[df['target']==1]['combined_text'], 20)\n",
    "common_words_bad2 = get_top_n_trigram(df[df['target']==0]['combined_text'], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(common_words_good2).to_csv(\"common_words_good2.csv\")\n",
    "pd.DataFrame(common_words_bad2).to_csv(\"common_words_bad2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,8))\n",
    "# good reviews bigrams\n",
    "plt.subplot(1,2,1)\n",
    "x_good2=[x[0] for x in common_words_good2]\n",
    "y_good2=[x[1] for x in common_words_good2]\n",
    "plt.bar(x_good2, y_good2, color='g')\n",
    "plt.xticks(rotation=90, fontsize=20)\n",
    "plt.title('Top 20 Trigrams in Negative Reviews', fontsize=20)\n",
    "for i in range(len(x_good2)):\n",
    "    plt.text(i-0.2, y_good2[i], '{}'.format(y_good2[i]), size=15, rotation=45)\n",
    "    \n",
    "plt.subplot(1,2,2)\n",
    "x_bad2=[x[0] for x in common_words_bad2]\n",
    "y_bad2=[x[1] for x in common_words_bad2]\n",
    "plt.bar(x_bad2, y_bad2, color='b')\n",
    "plt.xticks(rotation=90, fontsize=20)\n",
    "plt.title('Top 20 Trigrams in Nagative Reviews', fontsize=20)\n",
    "for i in range(len(x_bad2)):\n",
    "    plt.text(i-0.2, y_bad2[i], '{}'.format(y_bad2[i]), size=15, rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and more EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_list = stopwords.words('english')\n",
    "sw_list += list(string.punctuation)\n",
    "sw_list += [\"''\", '\"\"', '...', '``', '’', '“', '’', '”', '‘', '‘',\"'\", \n",
    "            '©','said',\"'s\", \"also\",'one',\"n't\",'com', \n",
    "            'satirewire', '-', '–', '—', '_','satirewire.com',\"/\"]\n",
    "sw_set = set(sw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "def process_review(review):\n",
    "    tokens = nltk.word_tokenize(review)  # tokenization\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in sw_set] #stop words removeal\n",
    "    return stopwords_removed\n",
    "\n",
    "# stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# create a functino stemming() and loop through each word in a review\n",
    "def stemming(review):\n",
    "    stemmed_review=[]\n",
    "    for w in review:\n",
    "        stemmed_review.append(ps.stem(w))\n",
    "    return stemmed_review\n",
    "\n",
    "# import libraries\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# create a functiion and loop through each word in a review\n",
    "def lemmatization(review):\n",
    "    lemma_list=[]\n",
    "    for word in review:\n",
    "        lemma_word = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemma_list.append(lemma_word)\n",
    "    return lemma_list\n",
    "\n",
    "# Combine all functions above and obtain cleaned text data\n",
    "def data_preprocessing(review):\n",
    "    # tokenization, stop words removal, punctuation marks removel\n",
    "    processed_review = list(map(process_review, review))\n",
    "    # stemming \n",
    "    stemming_reviews = list(map(stemming, processed_review))\n",
    "    # lemmatization\n",
    "    lemma_reviews = list(map(lemmatization, stemming_reviews))\n",
    "    return lemma_reviews\n",
    "\n",
    "cleaned_text_data=data_preprocessing(df['combined_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all functions above and obtain cleaned text data\n",
    "def data_preprocessing(review):\n",
    "    # tokenization, stop words removal, punctuation marks removal\n",
    "    processed_review = list(map(process_review, review))\n",
    "    \n",
    "    # stemming\n",
    "    stemming_reviews = list(map(stemming, processed_review))\n",
    "    \n",
    "    # lemmatization\n",
    "    lemma_reviews = list(map(lemmatization, stemming_reviews))\n",
    "    return lemma_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_data = data_preprocessing(df['combined_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_out = open('clean_text_data', 'wb')\n",
    "# pickle.dump(cleaned_text_data, pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_in_data = open(\"cleaned_text_data\", \"rb\")\n",
    "# cleaned_text_data = pickle.load(pickle_in_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two new columns one for filtered_review, one for review length\n",
    "df['filtered_review'] = cleaned_text_data\n",
    "df['review_len'] = [len(x) for x in df['filtered_review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the bad reviews and good reviews data and w\n",
    "badreviews = df[df.overall <= 3.0]['filtered_reivew']\n",
    "goodreviews = df[df.overall >= 4.0]['filtered_review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "#pd.DataFrame(pd.Series(chain(*goodreviews)).value_counts())\n",
    "bad_re = pd.DataFrame(pd.Series(chain(*badreviews)).value_counts()).sort_values(by=0, ascending=False).reset_index()\n",
    "good_re = pd.DataFrame(pd.Series(chain(*goodreviews)).value_counts()).sort_values(by=0, ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words in good reviews and bad reviews\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sns.barplot(good_re['index'][:20], good_re[0][:20])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Positive reviews', fontsize=20)\n",
    "plt.xlabel('Words', fontsize=15)\n",
    "plt.ylabel('Frequency', fontsize=15)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.barplot(bad_re['index'][:20], good_re[0][:20])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Negative reviews', fontsize=20)\n",
    "plt.xlabel('Words', fontsize=15)\n",
    "plt.ylabel('Frequency', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Length and Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('target')['review_len'].mean().plot(kinds='bar', color='b')\n",
    "plt.ylabel('Mean', fontsize=15)\n",
    "plt.xticks([0,1], ['Negative Review', 'Positive Review'],rotation=0)\n",
    "plt.title('Mean of Review Length by Rating', fontsize=20)\n",
    "plt.text(0-0.1, 46, '42.49')\n",
    "plt.show()\n",
    "# it makes sense the mean length of negative review is longer than positive reviews\n",
    "# 1, if people are not satisfied with the product they will speak a lot with their compaint\n",
    "# 2, people they satisfied or feel okey with the prduct they might not writing any reviews and the rating\n",
    "# will automatically become positive score or \n",
    "# will just say something \" good and well or I love it and like it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall\n",
    "plt.rcParams['figure.figsize'] = (10, 5)\n",
    "bins = 150\n",
    "plt.hist(df[df['target']==0]['review_len'], alpha=1, color='b', bins=bins, label='Nagative')\n",
    "plt.hist(df[df['target']==1]['review_len'], alpha=0.3, color='r', bins=bins, label='Positive')\n",
    "plt.xlabel('length')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlm(0, 200)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words_good = pd.read_csv('common_words_good.csv')\n",
    "common_words_bad = pd.read_csv('common_words_bad.csv')\n",
    "common_words_good.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
